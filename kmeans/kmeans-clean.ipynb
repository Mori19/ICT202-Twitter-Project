{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fffead22",
   "metadata": {},
   "source": [
    "# Using KMeans clustering to find Topics in Tweets about the Covid Vaccine\n",
    "\n",
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081542d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "##from sklearn.model_selection import GridSearchCV\n",
    "from datetime import datetime\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d09bee",
   "metadata": {},
   "source": [
    "### Gather the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07acc78",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "I choose to keep numbers for now, as they may fit into bigrams and provide some meaning\n",
    "Will delete any numbers that dont fit into bigrams later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703175c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "lm = WordNetLemmatizer()\n",
    "def remove_stop(text):\n",
    "    text = text.lower().split()\n",
    "    text = [word for word in text if word not in stop]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def get_processed_data():\n",
    "    processed_tweets = []\n",
    "    for tweet in df['content']:\n",
    "        prep_tweet = re.sub(r'(\\w+://\\S+)','',tweet.lower()) #remove links\n",
    "        prep_tweet = re.sub(r'&amp','',prep_tweet) #I saw a number of these; dont want them showing up\n",
    "        prep_tweet = re.sub(r'[^a-zA-Z0-9\\s]','',prep_tweet) #remove emojis and punctuation\n",
    "        prep_tweet = remove_stop(prep_tweet) #remove the stopwords - maybe this is too early\n",
    "        prep_tweet = ' '.join([lm.lemmatize(word) for word in prep_tweet.split(' ')]) #lemmatise the data\n",
    "        prep_tweet = prep_tweet.replace(' nh ',' nhs ') #I noticed this was being lemmatised, even though it is an important term\n",
    "        processed_tweets.append(prep_tweet)\n",
    "    return processed_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3007",
   "metadata": {},
   "source": [
    "### Need to cutdown the data in my set - Dont have enough RAM at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fee531",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = get_processed_tweets()[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdccf880",
   "metadata": {},
   "source": [
    "# Create bigrams and conduct more preprocessing\n",
    "\n",
    "## define the functions to preprocess the data\n",
    "\n",
    "### Consider moving this to a seperate python file - all the methods here are used by each modelling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts): #Train the model on our data, then return words based on our data. Iunno I dont make the rules here\n",
    "    bigram = Phrases(texts, min_count=5, threshold=100)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cace2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular_word_culler(doc_list): #doc_list is a list of strings\n",
    "    cvec = CountVectorizer(analyzer='word',       \n",
    "                                 min_df=10,\n",
    "                                 token_pattern='[\\w]{3,}',\n",
    "                                )\n",
    "    bow = cvec.fit_transform(doc_list)\n",
    "    bow_df = pd.DataFrame(bow.toarray(),columns=cvec.get_feature_names_out())\n",
    "    occurs = dict(zip([bow_df.T.iloc[x].name for x in range(len(bow_df.T))],\n",
    "                      [len(bow_df) - bow_df.T.iloc[x].to_list().count(0) for x in range(len(bow_df.T))]))\n",
    "    wanted_words = ['biden', 'boosted', 'booster', 'case', \n",
    "      'child', 'country', 'death', 'died', \n",
    "      'everyone', 'first', 'fully', 'good', \n",
    "      'health', 'kid', 'know', 'long', \n",
    "      'mandate', 'mask',  'new', 'pandemic', \n",
    "      'pfizer', 'rate', 'realcandaceo', \n",
    "      'risk', 'sorry', 'think', 'trump', 'work']\n",
    "    unwanted_words = [word for word in occurs.keys() if occurs[word] > 600 and word not in wanted_words]\n",
    "    new_tweets = []\n",
    "    for tweet in doc_list:\n",
    "        for word in unwanted_words:\n",
    "            tweet = tweet.replace(word,'')\n",
    "        new_tweets.append(tweet)\n",
    "    new_tweets_list = [[word for word in tweet.split()] for tweet in new_tweets]\n",
    "    return new_tweets, new_tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f473e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_word_culler(doc_list): #doc_list is a list of strings\n",
    "    long_word_tweets = []\n",
    "    for tweet in doc_list:\n",
    "        long_word_tweets.append(\n",
    "            ' '.join([word for word in tweet.split() if len(word)>3])\n",
    "        )\n",
    "\n",
    "    long_word_tweets_list = [[word for word in tweet.split()] for tweet in long_word_tweets]\n",
    "    return long_word_tweets, long_word_tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf4dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_culler(doc_list): #doc_list is a list of strings\n",
    "    a=[re.sub('\\b\\d+\\b','',doc) for doc in doc_list]\n",
    "    return a, [[word for word in doc.split()] for doc in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2de9f",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a84fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = make_bigrams(processed_tweets)\n",
    "processed_tweets = popular_word_culler(processed_tweets)\n",
    "processed_tweets = short_word_culler(processed_tweets)\n",
    "processed_tweets = number_culler(processed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d0813",
   "metadata": {},
   "source": [
    "# Create a dtm using TF\\*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4672c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vecs = tfidf.fit_transform(processed_tweets)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "dense = tfidf_vecs.todense()\n",
    "lst1 = dense.tolist()\n",
    "tfidf_df = pd.DataFrame(lst1, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898bcec",
   "metadata": {},
   "source": [
    "# K Means Clusstering of Tweets\n",
    "\n",
    "## Find the optimal number of topics through Elbow Method\n",
    "\n",
    "I have gone of the data that I used in the scratchpad - it is likely the results have changed since the processing method has also changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(5,80,5):\n",
    "    kmeans_model = KMeans(n_clusters=i, random_state=0)\n",
    "    kmeans_model.fit_transform(tfidf_vecs)\n",
    "    wcss.append(kmeans_model.inertia_)\n",
    "plt.plot(range(5,80,5),wcss)\n",
    "plt.title('elbow n clusters')\n",
    "plt.xlabel('n clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefafc2d",
   "metadata": {},
   "source": [
    "The graph has two noticable bends - fine tune through targeted graphing\n",
    "\n",
    "### Elbow Method - 10 to 20 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f56973",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(10,20,2):\n",
    "    kmeans_model = KMeans(n_clusters=i, random_state=0)\n",
    "    kmeans_model.fit_transform(tfidf_vecs)\n",
    "    wcss.append(kmeans_model.inertia_)\n",
    "plt.plot(range(10,20,2),wcss)\n",
    "plt.title('elbow n clusters')\n",
    "plt.xlabel('n clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00725355",
   "metadata": {},
   "source": [
    "## Display found topics  for 14 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6917262",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=14, random_state=0)\n",
    "kmeans_model.fit_transform(tfidf_vecs)\n",
    "order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]\n",
    "pd.DataFrame([[terms[i] for i in order_centroids[j]] for j in range(len(order_centroids))],\n",
    "                          index=[f'Topic {x}' for x in range(14)]).iloc[:,:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e009beb",
   "metadata": {},
   "source": [
    "### Elbow Method - 30 to 40 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd278140",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(30,40,2):\n",
    "    kmeans_model = KMeans(n_clusters=i, random_state=0)\n",
    "    kmeans_model.fit_transform(tfidf_vecs)\n",
    "    wcss.append(kmeans_model.inertia_)\n",
    "plt.plot(range(30,40,2),wcss)\n",
    "plt.title('elbow n clusters')\n",
    "plt.xlabel('n clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae9563",
   "metadata": {},
   "source": [
    "## Display found topics for 32 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=32, random_state=0)\n",
    "kmeans_model.fit_transform(tfidf_vecs)\n",
    "order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]\n",
    "pd.DataFrame([[terms[i] for i in order_centroids[j]] for j in range(len(order_centroids))],\n",
    "                          index=[f'Topic {x}' for x in range(32)]).iloc[:,:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb514689",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "## Make word clouds from intersting topics found in the above data\n",
    "\n",
    "There are too many topics to make word clouds for all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still need to work on this\n",
    "wc = WordCloud(max_words=100,width=2000,height=1000).generate_from_frequencies()\n",
    "\n",
    "#Figure out how to affect size of image - looks like extent but is more involved than I care to look at atm\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
